\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=1in}

\title{Homework 1 \\ CSC2515 Winter 2026}
\author{David}
\date{\today}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}

\begin{document}
\maketitle

\section*{Question 1: Training and Testing Error Curves}

This question illustrates the phenomena of underfitting and overfitting through idealized learning curves.

\subsection*{(a) Error vs. Model Complexity}

Figure~\ref{fig:complexity} shows training and testing error as a function of model complexity. As complexity increases, the training error decreases monotonically, since more expressive models can fit the training data more closely. In contrast, the testing error initially decreases as bias is reduced, reaches a minimum at an optimal complexity, and then increases due to overfitting and high variance.

The Bayes error represents the irreducible error inherent in the data-generating process and serves as a lower bound on achievable performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/complexity_curve.png}
    \caption{Training and testing error as a function of model complexity. The U-shaped test error curve illustrates the bias--variance tradeoff.}
    \label{fig:complexity}
\end{figure}

\subsection*{(b) Error vs. Training Set Size}

Figure~\ref{fig:datasize} depicts training and testing error as functions of the training set size for a fixed model complexity. As more data becomes available, the training error typically increases slightly, since fitting all data points perfectly becomes more difficult. Meanwhile, the testing error decreases as the model generalizes better.

Both curves converge to the same asymptotic error level, which lies above the Bayes error. This reflects the fact that, even with infinite data, a fixed-capacity model cannot outperform the Bayes-optimal classifier.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/datasize_curve.png}
    \caption{Training and testing error as a function of training set size. Increasing data reduces variance and improves generalization.}
    \label{fig:datasize}
\end{figure}

\section*{Question 2: Nearest Neighbours \& High Dimensions}

This question analyzes the behavior of squared Euclidean distances in high-dimensional spaces, illustrating the curse of dimensionality.

\subsection*{(a) One-dimensional case}

Let $X,Y \sim \mathrm{Unif}[0,1]$ be independent random variables, and define
\[
Z = (X-Y)^2.
\]

We first compute the expectation. By symmetry,
\[
\mathbb{E}[Z] = \int_0^1 \int_0^1 (x-y)^2 \, dx \, dy.
\]
Evaluating the inner integral,
\[
\int_0^1 (x-y)^2 dx = \int_0^1 (x^2 - 2xy + y^2) dx
= \frac{1}{3} - y + y^2.
\]
Integrating over $y$ gives
\[
\mathbb{E}[Z] = \int_0^1 \left( \frac{1}{3} - y + y^2 \right) dy
= \frac{1}{6}.
\]

Next, we compute the second moment:
\[
\mathbb{E}[Z^2] = \mathbb{E}[(X-Y)^4]
= \int_0^1 \int_0^1 (x-y)^4 \, dx \, dy.
\]
Evaluating yields
\[
\mathbb{E}[Z^2] = \frac{1}{15}.
\]
Therefore,
\[
\mathrm{Var}(Z) = \mathbb{E}[Z^2] - \mathbb{E}[Z]^2
= \frac{1}{15} - \left(\frac{1}{6}\right)^2
= \frac{7}{180}.
\]

\subsection*{(b) $d$-dimensional case}

Let $X,Y \in [0,1]^d$ with independent coordinates, and define the squared Euclidean distance
\[
R = \|X-Y\|_2^2 = \sum_{i=1}^d (X_i - Y_i)^2.
\]
Each term $(X_i-Y_i)^2$ is an independent copy of $Z$ from part (a). By linearity of expectation,
\[
\mathbb{E}[R] = \sum_{i=1}^d \mathbb{E}[Z] = \frac{d}{6}.
\]
Similarly, using independence,
\[
\mathrm{Var}(R) = \sum_{i=1}^d \mathrm{Var}(Z) = \frac{7d}{180}.
\]

\subsection*{(c) Interpretation in high dimensions}

The maximum possible squared distance between two points in the unit cube $[0,1]^d$ is attained at opposite corners and equals $d$. In contrast, the expected squared distance satisfies
\[
\mathbb{E}[R] = \frac{d}{6}.
\]
Thus, typical pairwise distances are a constant fraction of the maximum distance. Moreover, since the standard deviation of $R$ scales as $\sqrt{d}$ while the mean scales as $d$, the relative variability $\sqrt{\mathrm{Var}(R)}/\mathbb{E}[R]$ decays like $1/\sqrt{d}$. This concentration phenomenon implies that, in high dimensions, most points are approximately the same distance apart, illustrating the curse of dimensionality for nearest-neighbor methods.

\section*{Question 3: Information Theory Properties}

Throughout, let $X,Y$ be discrete random variables with probability mass functions $p(x)$ and $q(x)$.

\subsection*{(a) Entropy Non-negativity}

\begin{theorem}
For any discrete random variable $X$, the entropy satisfies $H(X) \ge 0$.
\end{theorem}

\begin{proof}
By definition,
\[
H(X) = -\sum_x p(x)\log p(x).
\]
For all $x$ with $p(x)>0$, we have $0 < p(x) \le 1$, hence $\log p(x) \le 0$ and so $- \log p(x) \ge 0$. Each term in the sum is therefore non-negative, implying $H(X) \ge 0$.
\end{proof}

\subsection*{(b) Chain Rule for Entropy}

\begin{theorem}
The joint entropy satisfies
\[
H(X,Y) = H(X) + H(Y \mid X).
\]
\end{theorem}

\begin{proof}
By definition of joint entropy,
\[
H(X,Y) = -\sum_{x,y} p(x,y)\log p(x,y).
\]
Using the factorization $p(x,y)=p(x)p(y\mid x)$,
\[
\begin{aligned}
H(X,Y)
&= -\sum_{x,y} p(x,y)\log\big(p(x)p(y\mid x)\big) \\
&= -\sum_{x,y} p(x,y)\log p(x) - \sum_{x,y} p(x,y)\log p(y\mid x).
\end{aligned}
\]
The first term simplifies to
\[
-\sum_x p(x)\log p(x) = H(X),
\]
and the second term is, by definition, $H(Y\mid X)$. Summing yields the desired identity.
\end{proof}

\subsection*{(c) Non-negativity of KL Divergence}

\begin{theorem}
For probability mass functions $p$ and $q$ on the same support,
\[
\mathrm{KL}(p\|q) = \sum_x p(x)\log\frac{p(x)}{q(x)} \ge 0.
\]
\end{theorem}

\begin{proof}
Rewrite the KL divergence as
\[
\mathrm{KL}(p\|q) = \mathbb{E}_p\!\left[ -\log\frac{q(X)}{p(X)} \right].
\]
The function $f(t)=-\log t$ is convex. By Jensen's inequality,
\[
\mathbb{E}_p[f(Z)] \ge f\big(\mathbb{E}_p[Z]\big),
\]
where $Z=\frac{q(X)}{p(X)}$. Therefore,
\[
\mathrm{KL}(p\|q) \ge -\log\!\left(\mathbb{E}_p\!\left[\frac{q(X)}{p(X)}\right]\right).
\]
But
\[
\mathbb{E}_p\!\left[\frac{q(X)}{p(X)}\right]
= \sum_x p(x)\frac{q(x)}{p(x)} = \sum_x q(x) = 1.
\]
Hence $\mathrm{KL}(p\|q) \ge -\log 1 = 0$.
\end{proof}

\subsection*{(d) Mutual Information as KL Divergence}

\begin{theorem}
The mutual information satisfies
\[
I(X;Y) = \mathrm{KL}\big(p(x,y)\,\|\,p(x)p(y)\big).
\]
\end{theorem}

\begin{proof}
By definition,
\[
I(X;Y) = \sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)}.
\]
This expression is exactly the KL divergence between the joint distribution $p(x,y)$ and the product of marginals $p(x)p(y)$. Hence,
\[
I(X;Y) = \mathrm{KL}\big(p(x,y)\,\|\,p(x)p(y)\big).
\]
\end{proof}

\section*{Question 4}
Figures generated from code will be inserted here.

\end{document}