\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=1in}

\title{Homework 1 \\ CSC2515 Winter 2026}
\author{David}
\date{\today}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}

\begin{document}
\maketitle


\section*{Question 2: Nearest Neighbours \& High Dimensions}

This question analyzes the behavior of squared Euclidean distances in high-dimensional spaces, illustrating the curse of dimensionality.

\subsection*{(a) One-dimensional case}

Let $X,Y \sim \mathrm{Unif}[0,1]$ be independent random variables, and define
\[
Z = (X-Y)^2.
\]

We first compute the expectation. By symmetry,
\[
\mathbb{E}[Z] = \int_0^1 \int_0^1 (x-y)^2 \, dx \, dy.
\]
Evaluating the inner integral,
\[
\int_0^1 (x-y)^2 dx = \int_0^1 (x^2 - 2xy + y^2) dx
= \frac{1}{3} - y + y^2.
\]
Integrating over $y$ gives
\[
\mathbb{E}[Z] = \int_0^1 \left( \frac{1}{3} - y + y^2 \right) dy
= \frac{1}{6}.
\]

Next, we compute the second moment:
\[
\mathbb{E}[Z^2] = \mathbb{E}[(X-Y)^4]
= \int_0^1 \int_0^1 (x-y)^4 \, dx \, dy.
\]
Evaluating yields
\[
\mathbb{E}[Z^2] = \frac{1}{15}.
\]
Therefore,
\[
\mathrm{Var}(Z) = \mathbb{E}[Z^2] - \mathbb{E}[Z]^2
= \frac{1}{15} - \left(\frac{1}{6}\right)^2
= \frac{7}{180}.
\]

\subsection*{(b) $d$-dimensional case}

Let $X,Y \in [0,1]^d$ with independent coordinates, and define the squared Euclidean distance
\[
R = \|X-Y\|_2^2 = \sum_{i=1}^d (X_i - Y_i)^2.
\]
Each term $(X_i-Y_i)^2$ is an independent copy of $Z$ from part (a). By linearity of expectation,
\[
\mathbb{E}[R] = \sum_{i=1}^d \mathbb{E}[Z] = \frac{d}{6}.
\]
Similarly, using independence,
\[
\mathrm{Var}(R) = \sum_{i=1}^d \mathrm{Var}(Z) = \frac{7d}{180}.
\]

\subsection*{(c) Interpretation in high dimensions}

The maximum possible squared distance between two points in the unit cube $[0,1]^d$ is attained at opposite corners and equals $d$. In contrast, the expected squared distance satisfies
\[
\mathbb{E}[R] = \frac{d}{6}.
\]
Thus, typical pairwise distances are a constant fraction of the maximum distance. Moreover, since the standard deviation of $R$ scales as $\sqrt{d}$ while the mean scales as $d$, the relative variability $\sqrt{\mathrm{Var}(R)}/\mathbb{E}[R]$ decays like $1/\sqrt{d}$. This concentration phenomenon implies that, in high dimensions, most points are approximately the same distance apart, illustrating the curse of dimensionality for nearest-neighbor methods.

\section*{Question 3: Information Theory Properties}

Throughout, let $X,Y$ be discrete random variables with probability mass functions $p(x)$ and $q(x)$.

\subsection*{(a) Entropy Non-negativity}

\begin{theorem}
For any discrete random variable $X$, the entropy satisfies $H(X) \ge 0$.
\end{theorem}

\begin{proof}
By definition,
\[
H(X) = -\sum_x p(x)\log p(x).
\]
For all $x$ with $p(x)>0$, we have $0 < p(x) \le 1$, hence $\log p(x) \le 0$ and so $- \log p(x) \ge 0$. Each term in the sum is therefore non-negative, implying $H(X) \ge 0$.
\end{proof}

\subsection*{(b) Chain Rule for Entropy}

\begin{theorem}
The joint entropy satisfies
\[
H(X,Y) = H(X) + H(Y \mid X).
\]
\end{theorem}

\begin{proof}
By definition of joint entropy,
\[
H(X,Y) = -\sum_{x,y} p(x,y)\log p(x,y).
\]
Using the factorization $p(x,y)=p(x)p(y\mid x)$,
\[
\begin{aligned}
H(X,Y)
&= -\sum_{x,y} p(x,y)\log\big(p(x)p(y\mid x)\big) \\
&= -\sum_{x,y} p(x,y)\log p(x) - \sum_{x,y} p(x,y)\log p(y\mid x).
\end{aligned}
\]
The first term simplifies to
\[
-\sum_x p(x)\log p(x) = H(X),
\]
and the second term is, by definition, $H(Y\mid X)$. Summing yields the desired identity.
\end{proof}

\subsection*{(c) Non-negativity of KL Divergence}

\begin{theorem}
For probability mass functions $p$ and $q$ on the same support,
\[
\mathrm{KL}(p\|q) = \sum_x p(x)\log\frac{p(x)}{q(x)} \ge 0.
\]
\end{theorem}

\begin{proof}
Rewrite the KL divergence as
\[
\mathrm{KL}(p\|q) = \mathbb{E}_p\!\left[ -\log\frac{q(X)}{p(X)} \right].
\]
The function $f(t)=-\log t$ is convex. By Jensen's inequality,
\[
\mathbb{E}_p[f(Z)] \ge f\big(\mathbb{E}_p[Z]\big),
\]
where $Z=\frac{q(X)}{p(X)}$. Therefore,
\[
\mathrm{KL}(p\|q) \ge -\log\!\left(\mathbb{E}_p\!\left[\frac{q(X)}{p(X)}\right]\right).
\]
But
\[
\mathbb{E}_p\!\left[\frac{q(X)}{p(X)}\right]
= \sum_x p(x)\frac{q(x)}{p(x)} = \sum_x q(x) = 1.
\]
Hence $\mathrm{KL}(p\|q) \ge -\log 1 = 0$.
\end{proof}

\subsection*{(d) Mutual Information as KL Divergence}

\begin{theorem}
The mutual information satisfies
\[
I(X;Y) = \mathrm{KL}\big(p(x,y)\,\|\,p(x)p(y)\big).
\]
\end{theorem}

\begin{proof}
By definition,
\[
I(X;Y) = \sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)}.
\]
This expression is exactly the KL divergence between the joint distribution $p(x,y)$ and the product of marginals $p(x)p(y)$. Hence,
\[
I(X;Y) = \mathrm{KL}\big(p(x,y)\,\|\,p(x)p(y)\big).
\]
\end{proof}

\section*{Question 4}
Figures generated from code will be inserted here.

\end{document}